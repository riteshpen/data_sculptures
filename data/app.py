# -*- coding: utf-8 -*-
"""data__sculpture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NXC-29DhOQUHpAv_7M-FK0hN6xLIANWQ

# ðŸ“Š Data Sculpture Essential Code
This notebook contains the minimal, relevant parts of your project for working with Firestore documents and using your TensorFlow model.

Trained Model
"""

import tensorflow as tf
from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D, Dense
from tensorflow.keras.models import Sequential, load_model
import numpy as np
import os

def train_sentiment_model(save_path='/content/drive/My Drive/Data Sculpture/sentiment_classifier.keras'):
    texts = [
        "Absolutely love using this app every day!", "Works well, no major issues.",
        "It's okay, some bugs but manageable.", "I'm frustrated with constant crashes.",
        "Terrible experience. Waste of time.", "Very helpful and intuitive interface.",
        "Not sure how I feel about the new update.", "App performance improved after update.",
        "The interface is ugly and confusing.", "I use it often, does what I need."
    ]
    labels = np.array([2, 2, 1, 0, 0, 2, 1, 2, 0, 2])  # 0=Neg, 1=Neutral, 2=Positive

    batch_size = 2
    max_features = 1000
    max_len = 20

    dataset = tf.data.Dataset.from_tensor_slices((texts, labels)).batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)

    vectorize_layer = TextVectorization(max_tokens=max_features, output_mode='int', output_sequence_length=max_len)
    vectorize_layer.adapt(texts)

    model = Sequential([
        vectorize_layer,
        Embedding(input_dim=max_features + 1, output_dim=16),
        GlobalAveragePooling1D(),
        Dense(32, activation='relu'),
        Dense(3, activation='softmax')
    ])

    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(dataset, epochs=10)

    # Make sure the parent directory exists before saving
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"Model saved to: {save_path}")

def load_sentiment_model(path='/content/drive/My Drive/Data Sculpture/sentiment_classifier.keras'):
    return load_model(path)

def predict_sentiment(text, model):
    text_tensor = tf.constant([text])
    probs = model.predict(text_tensor)[0]
    classes = ['Negative', 'Neutral', 'Positive']
    return dict(zip(classes, np.round(probs, 3)))

if __name__ == "__main__":
    train_sentiment_model()

# prompt: connect to google drive
from google.colab import drive
drive.mount('/content/drive')

!pip install streamlit

# =====================================
# 0. Imports and Environment Setup
# =====================================
import streamlit as st
import pandas as pd
import json
import os
import re
import tensorflow as tf
from scipy.stats import ttest_ind, chi2_contingency, pearsonr, skew, kurtosis
from google.cloud import firestore

# Set up GCP credentials (update path if needed)
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/drive/My Drive/Data Sculpture/data-sculpture-ai-firebase-adminsdk-4ebrf-5409e1f2b5.json"

# =====================================
# 1. Firestore Data Fetching
# =====================================
def fetch_firestore_data(collection_name="library"):
    """Fetch documents from Firestore and return as list of dicts"""
    db = firestore.Client()
    docs = db.collection(collection_name).stream()
    results = []
    for doc in docs:
        data = doc.to_dict()
        results.append(data)
    return results


# =====================================
# 2. Vizualization Rules
# =====================================

VISUALIZATION_RULES = {
    'numeric_vs_numeric': {
        'chart_types': ['scatter', 'line', 'bubble'],
        'color_palettes': ['continuous', 'gradient']
    },
    'categorical_vs_numeric': {
        'chart_types': ['bar', 'box'],
        'color_palettes': ['qualitative']
    },
    'categorical_vs_categorical': {
        'chart_types': ['heatmap'],
        'color_palettes': ['diverging']
    },
    'time_series': {
        'chart_types': ['line', 'area'],
        'color_palettes': ['sequential']
    }
}

def recommend_visual(structure):
    if structure['temporal_columns'] and structure['numeric_columns']:
        return VISUALIZATION_RULES['time_series']
    elif len(structure['numeric_columns']) >= 2:
        return VISUALIZATION_RULES['numeric_vs_numeric']
    elif structure['numeric_columns'] and structure['categorical_columns']:
        return VISUALIZATION_RULES['categorical_vs_numeric']
    elif len(structure['categorical_columns']) >= 2:
        return VISUALIZATION_RULES['categorical_vs_categorical']
    else:
        return {'chart_types': ['table'], 'color_palettes': ['default']}


# =====================================
# 3. Flatten Nested Dictionaries
# =====================================
def flatten_dict_items(data_items):
    """Flatten nested dictionaries in a list of dicts"""
    flat_data = []
    for item in data_items:
        if isinstance(item, dict):
            flat_row = {}
            for k, v in item.items():
                if isinstance(v, dict):
                    for sub_k, sub_v in v.items():
                        flat_row[sub_k] = sub_v
                else:
                    flat_row[k] = v
            flat_data.append(flat_row)
    return flat_data

import os
from google.cloud import firestore

db = firestore.Client()

# Document 1: enhanced_results_test_01
doc1 = {
    "feature1": 8.4,
    "feature2": 15.2,
    "enhanced_summary": {
        "additional_note": "Testing feature-to-feature relation in neutral context.",
        "category": "NeutralTest",
        "length": 58,
        "local_sentiment_prediction": "Neutral",
        "snippet": "This entry serves to validate how neutral sentiment is handled."
    },
    "statistical_analysis_results": {
        "associations": [
            {
                "column_pair": "feature1 vs feature2",
                "interpretation": "Moderate correlation observed, not statistically significant",
                "test": "Pearson"
            },
            {
                "column_pair": "feature5 vs feature6",
                "interpretation": "No significant association detected",
                "test": "Chi-square"
            }
        ],
        "summary": "Neutral example to ensure proper sentiment and statistical classification."
    },
    "visualization_recommendations": {
        "recommended_chart": "Scatter Plot",
        "recommended_palette": "Blue-Gray Scale"
    }
}

# Document 2: enhanced_results_test_02
doc2 = {
    "feature1": 4.7,
    "feature2": 3.1,
    "enhanced_summary": {
        "additional_note": "This document helps test negative tone detection.",
        "category": "NegativeTest",
        "length": 49,
        "local_sentiment_prediction": "Negative",
        "snippet": "System seems unreliable and response times are too slow."
    },
    "statistical_analysis_results": {
        "associations": [
            {
                "column_pair": "feature1 vs feature2",
                "interpretation": "Weak negative correlation observed",
                "test": "Pearson"
            },
            {
                "column_pair": "featureA vs featureB",
                "interpretation": "Potential association; p-value just above threshold",
                "test": "Chi-square"
            }
        ],
        "summary": "Negative tone validated with supporting feature-level stats."
    },
    "visualization_recommendations": {
        "recommended_chart": "Bar Chart",
        "recommended_palette": "Red-Yellow Scale"
    }
}

# Add both documents to the "library" collection
db.collection("library").document("enhanced_results_test_01").set(doc1)
db.collection("library").document("enhanced_results_test_02").set(doc2)

print("âœ… Documents successfully uploaded to Firestore.")

# =====================================
# 4. Data Structure Analysis & Chart Detection
# =====================================
def analyze_data_structure(data_items):
    """Detect data types, patterns, and characteristics"""
    structure = {
        'numeric_columns': [],
        'categorical_columns': [],
        'temporal_columns': [],
        'has_baseline': False,
        'data_distribution': {},
        'relationships': []
    }

    try:
        flat_data = flatten_dict_items(data_items)
        df = pd.DataFrame(flat_data)
        if df.empty:
            return structure

        for col in df.columns:
            col_data = df[col].dropna()

            # Detect temporal columns
            try:
                parsed_dates = pd.to_datetime(col_data, errors='coerce')
                if parsed_dates.notna().sum() > 0.5 * len(col_data):
                    structure['temporal_columns'].append(col)
                    continue
            except:
                pass

            # Detect numeric columns
            if pd.api.types.is_numeric_dtype(col_data):
                structure['numeric_columns'].append(col)
                structure['data_distribution'][col] = {
                    "mean": float(col_data.mean()),
                    "std": float(col_data.std()),
                    "min": float(col_data.min()),
                    "max": float(col_data.max())
                }

            # Detect categorical columns
            elif pd.api.types.is_object_dtype(col_data) or col_data.nunique() < 10:
                structure['categorical_columns'].append(col)
                if col_data.nunique() == 2:
                    structure['has_baseline'] = True

        # Compute correlations between numeric columns
        num_cols = structure['numeric_columns']
        for i in range(len(num_cols)):
            for j in range(i + 1, len(num_cols)):
                col1, col2 = num_cols[i], num_cols[j]
                try:
                    r, p = pearsonr(df[col1].dropna(), df[col2].dropna())
                    if p < 0.05:
                        structure['relationships'].append({
                            "columns": (col1, col2),
                            "correlation": round(r, 3),
                            "p_value": round(p, 4)
                        })
                except:
                    continue

    except Exception as e:
        print("Error analyzing data structure:", e)

    return structure

def detect_chart_context(structure):
    """Determine recommended chart type from data structure"""
    context = {"recommended_chart": None, "purpose": None, "reason": ""}
    num, cat, time = structure['numeric_columns'], structure['categorical_columns'], structure['temporal_columns']

    if time and num:
        context.update({
            "recommended_chart": "line",
            "purpose": "trend over time",
            "reason": f"Time column '{time[0]}' and numeric column '{num[0]}' detected"
        })
    elif len(num) >= 2:
        context.update({
            "recommended_chart": "scatter",
            "purpose": "correlation",
            "reason": f"Two numeric columns '{num[0]}' and '{num[1]}' detected"
        })
    elif cat and num:
        context.update({
            "recommended_chart": "bar",
            "purpose": "comparison",
            "reason": f"Categorical column '{cat[0]}' and numeric column '{num[0]}' detected"
        })
    elif len(cat) >= 2:
        context.update({
            "recommended_chart": "heatmap",
            "purpose": "cross-category analysis",
            "reason": f"Two categorical columns '{cat[0]}' and '{cat[1]}' detected"
        })
    elif num:
        context.update({
            "recommended_chart": "histogram",
            "purpose": "distribution",
            "reason": f"One numeric column '{num[0]}' detected"
        })
    elif cat:
        context.update({
            "recommended_chart": "pie",
            "purpose": "proportion",
            "reason": f"One categorical column '{cat[0]}' detected"
        })
    else:
        context.update({
            "recommended_chart": "table",
            "purpose": "raw data",
            "reason": "Unable to infer a strong visualization structure"
        })

    return context

# =====================================
# 5. Advanced Statistical Analysis
# =====================================
def advanced_statistical_analysis(data, analysis_type=None):
    """Perform association and basic stats analysis"""
    results = {"associations": [], "summary": "", "basic_stats": {}}

    try:
        df = pd.DataFrame(flatten_dict_items(data))
        if df.empty:
            results["summary"] = "No valid data found for analysis."
            return results

        num_cols = df.select_dtypes(include=['number']).columns.tolist()
        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

        # Basic stats for numeric columns
        for col in num_cols:
            col_data = df[col].dropna()
            if not col_data.empty:
                results["basic_stats"][col] = {
                    "mean": float(col_data.mean()),
                    "variance": float(col_data.var()),
                    "skewness": float(skew(col_data)) if len(col_data) > 1 else 0.0,
                    "kurtosis": float(kurtosis(col_data)) if len(col_data) > 1 else 0.0
                }

        # Association analysis
        for i, col1 in enumerate(df.columns):
            for col2 in df.columns[i+1:]:
                if col1 == col2 or df[col1].nunique() < 2 or df[col2].nunique() < 2:
                    continue
                pair_result = {"column_pair": (col1, col2), "test": None, "p_value": None, "significant": False, "interpretation": ""}
                try:
                    if col1 in num_cols and col2 in num_cols:
                        temp_df = df[[col1, col2]].dropna()
                        if len(temp_df) > 1:
                            corr, p = pearsonr(temp_df[col1], temp_df[col2])
                            pair_result.update({
                                "test": "Pearson Correlation",
                                "p_value": round(p, 4),
                                "significant": p < 0.05,
                                "interpretation": f"{col1} and {col2} {'have significant' if p < 0.05 else 'show weak'} linear correlation."
                            })
                    elif (col1 in num_cols and col2 in cat_cols) or (col2 in num_cols and col1 in cat_cols):
                        num_col, cat_col = (col1, col2) if col1 in num_cols else (col2, col1)
                        groups = [df[df[cat_col] == val][num_col].dropna() for val in df[cat_col].unique()]
                        valid_groups = [g for g in groups if len(g) >= 2]
                        if len(valid_groups) == 2:
                            stat, p = ttest_ind(valid_groups[0], valid_groups[1], equal_var=False)
                            pair_result.update({
                                "test": "T-Test",
                                "p_value": round(p, 4),
                                "significant": p < 0.05,
                                "interpretation": f"Mean of {num_col} differs across {cat_col} groups (p={round(p,4)})."
                            })
                    elif col1 in cat_cols and col2 in cat_cols:
                        contingency = pd.crosstab(df[col1], df[col2])
                        if contingency.shape[0] > 1 and contingency.shape[1] > 1:
                            stat, p, _, _ = chi2_contingency(contingency)
                            pair_result.update({
                                "test": "Chi-Square",
                                "p_value": round(p, 4),
                                "significant": p < 0.05,
                                "interpretation": f"{col1} and {col2} are {'associated' if p < 0.05 else 'not significantly associated'} (p={round(p,4)})."
                            })
                except Exception:
                    continue
                if pair_result["test"]:
                    results["associations"].append(pair_result)

        sig = [r for r in results["associations"] if r["significant"]]
        results["summary"] = f"Found {len(sig)} significant relationships out of {len(results['associations'])} variable pairs." if results["associations"] else "No strong statistical relationships found."
        return results

    except Exception as e:
        results["summary"] = f"Analysis failed: {e}"
        return results

# =====================================
# 6. Streamlit App Interface
# =====================================
# Save the full code to app.py
from google.colab import files

with open('app.py', 'w') as f:
    f.write("""
# All your imports here
import streamlit as st
import pandas as pd
import json
import os
import re
import tensorflow as tf
from scipy.stats import ttest_ind, chi2_contingency, pearsonr, skew, kurtosis
from google.cloud import firestore
st.title("ðŸ“Š Data Sculpture: Interactive Data Analysis & Visualization Assistant")

st.write("This tool fetches Firestore data, analyzes its structure, performs advanced statistical tests, and recommends visualizations tailored to your dataset.")

if st.button("Fetch Data from Firestore"):
    with st.spinner("Fetching data..."):
        data = fetch_firestore_data()
    if data:
        st.success(f"Fetched {len(data)} documents.")
        df = pd.DataFrame(flatten_dict_items(data))
        st.write("Raw Data Preview:", df.head())

        with st.spinner("Analyzing data structure..."):
            structure = analyze_data_structure(data)
            chart_context = detect_chart_context(structure)
        st.write("ðŸ”Ž Data Structure Analysis:", structure)
        st.info(f"ðŸ“ˆ Recommended Chart: **{chart_context['recommended_chart']}**")
        st.caption(f"Purpose: {chart_context['purpose']} | Reason: {chart_context['reason']}")

        with st.spinner("Performing statistical tests..."):
            stats = advanced_statistical_analysis(data)
        st.write("ðŸ“Š Advanced Statistical Analysis:", stats)

    else:
        st.error("No data retrieved or collection is empty.")


""")

files.download('app.py')

# 2. Install required packages
!pip install -q streamlit pyngrok

!ngrok authtoken 2kerBbPvj3YlFUTQTjDwDg8qSQt_2Bb42u5oNy8FrwSYYDkdN

!netstat -tulnp 2>/dev/null | grep 8501  # Check if something is listening on 8501

!ps aux | grep streamlit
!cat /root/.streamlit/logs/st.log

!curl http://localhost:8501

# =====================================
# 0. Imports and Environment Setup
# =====================================
import streamlit as st
import pandas as pd
import os
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency, pearsonr, skew, kurtosis
from google.cloud import firestore
import time
from pyngrok import ngrok
import inspect

# Set up GCP credentials
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/drive/My Drive/Data Sculpture/data-sculpture-ai-firebase-adminsdk-4ebrf-5409e1f2b5.json"

# =====================================
# 1. Firestore Data Fetching
# =====================================
def fetch_firestore_data(collection_name="library"):
    """Fetch documents from Firestore and return as list of dicts"""
    db = firestore.Client()
    docs = db.collection(collection_name).stream()
    return [doc.to_dict() for doc in docs]

# =====================================
# 2. Flatten Nested Dictionaries
# =====================================
def flatten_dict_items(data_items):
    """Flatten nested dictionaries in a list of dicts"""
    flat_data = []
    for item in data_items:
        if isinstance(item, dict):
            flat_row = {}
            for k, v in item.items():
                if isinstance(v, dict):
                    for sub_k, sub_v in v.items():
                        flat_row[f"{k}_{sub_k}"] = sub_v
                else:
                    flat_row[k] = v
            flat_data.append(flat_row)
    return flat_data

# =====================================
# 3. Data Structure Analysis & Chart Detection
# =====================================
def analyze_data_structure(data_items):
    """Detect data types, patterns, and characteristics"""
    structure = {
        'numeric_columns': [],
        'categorical_columns': [],
        'temporal_columns': [],
        'has_baseline': False,
        'data_distribution': {},
        'relationships': []
    }

    try:
        df = pd.DataFrame(flatten_dict_items(data_items))
        if df.empty:
            return structure

        for col in df.columns:
            col_data = df[col].dropna()

            # Detect temporal columns
            if pd.api.types.is_datetime64_any_dtype(col_data) or (
                pd.to_datetime(col_data, errors='coerce').notna().mean() > 0.5
            ):
                structure['temporal_columns'].append(col)
                continue

            # Detect numeric columns
            if pd.api.types.is_numeric_dtype(col_data):
                structure['numeric_columns'].append(col)
                structure['data_distribution'][col] = {
                    "mean": float(col_data.mean()),
                    "std": float(col_data.std()),
                    "min": float(col_data.min()),
                    "max": float(col_data.max())
                }
            # Detect categorical columns
            elif pd.api.types.is_string_dtype(col_data) or col_data.nunique() < 10:
                structure['categorical_columns'].append(col)
                if col_data.nunique() == 2:
                    structure['has_baseline'] = True

        # Compute correlations
        num_cols = structure['numeric_columns']
        for i in range(len(num_cols)):
            for j in range(i + 1, len(num_cols)):
                col1, col2 = num_cols[i], num_cols[j]
                try:
                    r, p = pearsonr(df[col1].dropna(), df[col2].dropna())
                    if p < 0.05:
                        structure['relationships'].append({
                            "columns": (col1, col2),
                            "correlation": round(r, 3),
                            "p_value": round(p, 4)
                        })
                except:
                    continue

    except Exception as e:
        st.error(f"Error analyzing data structure: {e}")

    return structure

def detect_chart_context(structure):
    """Determine recommended chart type from data structure"""
    context = {
        "recommended_chart": "table",
        "purpose": "raw data",
        "reason": "Unable to infer a strong visualization structure"
    }
    num, cat, time = structure['numeric_columns'], structure['categorical_columns'], structure['temporal_columns']

    if time and num:
        context.update({
            "recommended_chart": "line",
            "purpose": "trend over time",
            "reason": f"Time column '{time[0]}' and numeric column '{num[0]}' detected"
        })
    elif len(num) >= 2:
        context.update({
            "recommended_chart": "scatter",
            "purpose": "correlation",
            "reason": f"Two numeric columns '{num[0]}' and '{num[1]}' detected"
        })
    elif cat and num:
        context.update({
            "recommended_chart": "bar",
            "purpose": "comparison",
            "reason": f"Categorical column '{cat[0]}' and numeric column '{num[0]}' detected"
        })
    elif len(cat) >= 2:
        context.update({
            "recommended_chart": "heatmap",
            "purpose": "cross-category analysis",
            "reason": f"Two categorical columns '{cat[0]}' and '{cat[1]}' detected"
        })
    elif num:
        context.update({
            "recommended_chart": "histogram",
            "purpose": "distribution",
            "reason": f"One numeric column '{num[0]}' detected"
        })
    elif cat:
        context.update({
            "recommended_chart": "pie",
            "purpose": "proportion",
            "reason": f"One categorical column '{cat[0]}' detected"
        })

    return context

# =====================================
# 4. Advanced Statistical Analysis
# =====================================
def advanced_statistical_analysis(data):
    """Perform statistical analysis on the data"""
    results = {"associations": [], "summary": "", "basic_stats": {}}

    try:
        df = pd.DataFrame(flatten_dict_items(data))
        if df.empty:
            results["summary"] = "No valid data found for analysis."
            return results

        # Basic stats
        num_cols = df.select_dtypes(include=np.number).columns
        for col in num_cols:
            col_data = df[col].dropna()
            if not col_data.empty:
                results["basic_stats"][col] = {
                    "mean": float(col_data.mean()),
                    "std": float(col_data.std()),
                    "skewness": float(skew(col_data)) if len(col_data) > 2 else None,
                    "kurtosis": float(kurtosis(col_data)) if len(col_data) > 2 else None
                }


        # Association analysis
        for i, col1 in enumerate(df.columns):
            for col2 in df.columns[i+1:]:
                if df[col1].nunique() < 2 or df[col2].nunique() < 2:
                    continue

                test_result = {
                    "columns": (col1, col2),
                    "test": None,
                    "p_value": None,
                    "significant": False,
                    "interpretation": ""
                }

                # Numeric vs Numeric
                if col1 in num_cols and col2 in num_cols:
                    temp_df = df[[col1, col2]].dropna()
                    if len(temp_df) > 1:
                        r, p = pearsonr(temp_df[col1], temp_df[col2])
                        test_result.update({
                            "test": "Pearson r",
                            "p_value": round(p, 4),
                            "significant": p < 0.05,
                            "interpretation": f"Correlation: {round(r, 2)} (p={round(p,4)})"
                        })

                # Numeric vs Categorical
                elif (col1 in num_cols) or (col2 in num_cols):
                    num_col = col1 if col1 in num_cols else col2
                    cat_col = col2 if col1 in num_cols else col1
                    groups = [group.dropna() for name, group in df.groupby(cat_col)[num_col] if len(group.dropna()) > 1]

                    if len(groups) == 2:
                        try:
                            t, p = ttest_ind(groups[0], groups[1], equal_var=False)
                            test_result.update({
                                "test": "T-test",
                                "p_value": round(p, 4),
                                "significant": p < 0.05,
                                "interpretation": f"Mean difference: {round(groups[0].mean()-groups[1].mean(),2) if not groups[0].empty and not groups[1].empty else 'N/A'} (p={round(p,4)})"
                            })
                        except ValueError:
                            pass # Handle cases where ttest_ind fails
                    elif len(groups) > 2:
                         # Could potentially add ANOVA here if needed for more than 2 groups
                        pass


                # Categorical vs Categorical
                else:
                    contingency = pd.crosstab(df[col1], df[col2])
                    if contingency.size > 1 and not contingency.empty:
                        try:
                            chi2, p, _, _ = chi2_contingency(contingency)
                            test_result.update({
                                "test": "Chi-square",
                                "p_value": round(p, 4),
                                "significant": p < 0.05,
                                "interpretation": f"Association (p={round(p,4)})"
                            })
                        except ValueError:
                             pass # Handle cases where chi2_contingency fails


                if test_result["test"]:
                    results["associations"].append(test_result)

        # Generate summary
        sig_results = sum(1 for r in results["associations"] if r["significant"])
        results["summary"] = (
            f"Found {sig_results} significant relationships "
            f"out of {len(results['associations'])} tested pairs."
        )

    except Exception as e:
        results["summary"] = f"Analysis failed: {str(e)}"

    return results

# =====================================
# 5. Streamlit App Interface
# =====================================
def main():
    st.set_page_config(page_title="Data Sculpture", layout="wide")

    st.title("ðŸ“Š Data Sculpture: Interactive Data Analysis & Visualization Assistant")
    st.write("""
    This tool fetches Firestore data, analyzes its structure,
    performs advanced statistical tests, and recommends visualizations.
    """)

    if st.button("Fetch Data from Firestore"):
        with st.spinner("Fetching data..."):
            data = fetch_firestore_data()

        if not data:
            st.error("No data retrieved or collection is empty.")
            return

        st.success(f"Fetched {len(data)} documents.")
        df = pd.DataFrame(flatten_dict_items(data))

        # Show raw data
        with st.expander("View Raw Data"):
            st.dataframe(df.head())
            st.download_button(
                "Download Data",
                df.to_csv(index=False),
                "firestore_data.csv",
                "text/csv"
            )

        # Data structure analysis
        with st.spinner("Analyzing data structure..."):
            structure = analyze_data_structure(data)
            chart_rec = detect_chart_context(structure)

        col1, col2 = st.columns(2)
        with col1:
            st.subheader("Data Structure Analysis")
            st.json(structure)
        with col2:
            st.subheader("Recommended Visualization")
            st.info(f"**{chart_rec['recommended_chart'].upper()}** chart")
            st.caption(f"**Purpose:** {chart_rec['purpose']}")
            st.caption(f"**Reason:** {chart_rec['reason']}")

        # Statistical analysis
        with st.spinner("Performing statistical tests..."):
            stats = advanced_statistical_analysis(data)

        st.subheader("Statistical Analysis Results")
        st.write(stats["summary"])

        if stats["associations"]:
            st.dataframe(pd.DataFrame(stats["associations"]))

# =====================================
# 6. Run the App with Ngrok
# =====================================

# Write the complete app to a file
# Collect all function definitions
app_code = inspect.getsource(main) + "\n\n"
functions_to_write = [
    fetch_firestore_data,
    flatten_dict_items,
    analyze_data_structure,
    detect_chart_context,
    advanced_statistical_analysis
]
for func in functions_to_write:
    app_code += inspect.getsource(func) + "\n\n"

with open('streamlit_app.py', 'w') as f:
    f.write(app_code)

# Kill any existing processes
!pkill -f streamlit
ngrok.kill()

# Set up ngrok
NGROK_AUTH_TOKEN = "2kerBbPvj3YlFUTQTjDwDg8qSQt_2Bb42u5oNy8FrwSYYDkdN"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Launch Streamlit with nohup
!nohup streamlit run streamlit_app.py --server.port 8501 --server.headless true > /dev/null 2>&1 &

# Wait longer for Colab
time.sleep(20)

# Create tunnel
try:
    public_url = ngrok.connect(8501, bind_tls=True)
    print(f"\nðŸ”— Your app is live at: {public_url}")
    print("âš ï¸ If you get connection errors:")
    print("1. Try increasing the sleep time (currently 20 seconds)")
    print("2. Check if Streamlit is running: !ps aux | grep streamlit")
    print("3. Try accessing locally first: !curl http://localhost:8501")
except Exception as e:
    print(f"Error creating ngrok tunnel: {e}")
    print("Please check the troubleshooting steps above.")

# ========================
# 1. Imports & Setup
# ========================
import os
import json
import re
import streamlit as st
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency, pearsonr, skew, kurtosis
import tensorflow as tf
from tensorflow.keras.models import load_model

# Set credentials for Firestore (update path as needed)
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/drive/My Drive/Data Sculpture/data-sculpture-ai-firebase-adminsdk-4ebrf-5409e1f2b5.json"

# ========================
# 2. Firestore Integration
# ========================
from google.cloud import firestore

def fetch_firestore_data(collection_name="library"):
    db = firestore.Client()
    docs = db.collection(collection_name).stream()
    return [doc.to_dict() for doc in docs]

def flatten_dict_items(data_items):
    flat_data = []
    for item in data_items:
        if isinstance(item, dict):
            flat_row = {}
            for k, v in item.items():
                if isinstance(v, dict):
                    for sub_k, sub_v in v.items():
                        flat_row[sub_k] = sub_v
                else:
                    flat_row[k] = v
            flat_data.append(flat_row)
    return flat_data

# ========================
# 3. Data Structure Analysis
# ========================
def analyze_data_structure(data_items):
    structure = {
        'numeric_columns': [],
        'categorical_columns': [],
        'temporal_columns': [],
        'has_baseline': False,
        'data_distribution': {},
        'relationships': []
    }

    flat_data = flatten_dict_items(data_items)
    df = pd.DataFrame(flat_data)
    if df.empty:
        return structure

    for col in df.columns:
        col_data = df[col].dropna()
        try:
            parsed_dates = pd.to_datetime(col_data, errors='coerce')
            if parsed_dates.notna().sum() > 0.5 * len(col_data):
                structure['temporal_columns'].append(col)
                continue
        except:
            pass

        if pd.api.types.is_numeric_dtype(col_data):
            structure['numeric_columns'].append(col)
            structure['data_distribution'][col] = {
                "mean": float(col_data.mean()),
                "std": float(col_data.std()),
                "min": float(col_data.min()),
                "max": float(col_data.max())
            }
        elif pd.api.types.is_object_dtype(col_data) or col_data.nunique() < 10:
            structure['categorical_columns'].append(col)
            if col_data.apply(lambda x: isinstance(x, list)).any():
              continue

    num_cols = structure['numeric_columns']
    for i in range(len(num_cols)):
        for j in range(i + 1, len(num_cols)):
            col1, col2 = num_cols[i], num_cols[j]
            try:
                r, p = pearsonr(df[col1].dropna(), df[col2].dropna())
                if p < 0.05:
                    structure['relationships'].append({
                        "columns": (col1, col2),
                        "correlation": round(r, 3),
                        "p_value": round(p, 4)
                    })
            except:
                continue

    return structure

# ========================
# 4. Visualization Recommendation
# ========================
def detect_chart_context(structure):
    num = structure['numeric_columns']
    cat = structure['categorical_columns']
    time = structure['temporal_columns']
    rels = structure['relationships']

    context = {
        "recommended_chart": None,
        "purpose": None,
        "reason": ""
    }

    if time and num:
        context["recommended_chart"] = "line"
        context["purpose"] = "trend over time"
        context["reason"] = f"Time column '{time[0]}' and numeric column '{num[0]}' detected"

    elif len(num) >= 2:
        context["recommended_chart"] = "scatter"
        context["purpose"] = "correlation"
        context["reason"] = f"Two numeric columns '{num[0]}' and '{num[1]}' detected"

    elif cat and num:
        context["recommended_chart"] = "bar"
        context["purpose"] = "comparison"
        context["reason"] = f"Categorical column '{cat[0]}' and numeric column '{num[0]}' detected"

    elif len(cat) >= 2:
        context["recommended_chart"] = "heatmap"
        context["purpose"] = "cross-category analysis"
        context["reason"] = f"Two categorical columns '{cat[0]}' and '{cat[1]}' detected"

    elif num:
        context["recommended_chart"] = "histogram"
        context["purpose"] = "distribution"
        context["reason"] = f"One numeric column '{num[0]}' detected"

    elif cat:
        context["recommended_chart"] = "pie"
        context["purpose"] = "proportion"
        context["reason"] = f"One categorical column '{cat[0]}' detected"

    else:
        context["recommended_chart"] = "table"
        context["purpose"] = "raw data"
        context["reason"] = "Unable to infer a strong visualization structure"

    return context

# ========================
# 5. Statistical Tests
# ========================
def perform_statistical_tests(data_items):
    try:
        df = pd.DataFrame(flatten_dict_items(data_items))
        result = {}

        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

        for i, col1 in enumerate(df.columns):
            for col2 in df.columns[i+1:]:
                if col1 == col2 or df[col1].nunique() < 2 or df[col2].nunique() < 2:
                    continue

                if col1 in numeric_cols and col2 in numeric_cols:
                    stat, p = pearsonr(df[col1].dropna(), df[col2].dropna())
                    result[f"{col1} vs {col2}"] = {
                        "test": "Pearson Correlation",
                        "p_value": round(p, 4),
                        "significant": p < 0.05
                    }

                elif col1 in numeric_cols and col2 in categorical_cols:
                    groups = [df[df[col2] == val][col1].dropna() for val in df[col2].unique()]
                    if len(groups) == 2:
                        stat, p = ttest_ind(groups[0], groups[1], equal_var=False)
                        result[f"{col1} ~ {col2}"] = {
                            "test": "T-Test",
                            "p_value": round(p, 4),
                            "significant": p < 0.05
                        }

                elif col1 in categorical_cols and col2 in categorical_cols:
                    contingency = pd.crosstab(df[col1], df[col2])
                    if contingency.shape[0] > 1 and contingency.shape[1] > 1:
                        stat, p, _, _ = chi2_contingency(contingency)
                        result[f"{col1} vs {col2}"] = {
                            "test": "Chi-Square",
                            "p_value": round(p, 4),
                            "significant": p < 0.05
                        }

        return result
    except Exception as e:
        return {"error": str(e)}

# ========================
# 6. Sentiment Classifier
# ========================
def load_sentiment_model(path='/content/drive/My Drive/Data Sculpture/sentiment_classifier.keras'):
    try:
        import os
        model_path = "sentiment_classifier.keras"
        if os.path.exists(model_path):
          import os
          model_path = "sentiment_classifier.keras"
          if os.path.exists(model_path):
              model = tf.keras.models.load_model(model_path)
          else:
              st.warning("âš ï¸ Sentiment model not found. Skipping sentiment analysis section.")
              model = None

        else:
          st.warning("âš ï¸ Sentiment model not found. Skipping sentiment analysis section.")
          model = None
    except Exception as e:
        st.error(f"Model load error: {e}")
        return None

def predict_sentiment(model, text):
    text_tensor = tf.constant([text])
    probs = model.predict(text_tensor)[0]
    classes = ['Negative', 'Neutral', 'Positive']
    return dict(zip(classes, np.round(probs, 3)))

# ========================
# 7. Streamlit App
# ========================
def main():
    st.title("ðŸ§  Data Sculpture: Statistical Insight & Visualization Engine")

    # Load sentiment model
    model = load_sentiment_model()

    # Section 1: Firestore Data Fetch
    if st.button("Fetch Firestore Data"):
        data = fetch_firestore_data()
        st.success(f"Fetched {len(data)} documents.")
        st.json(data)

        # Section 2: Structure
        structure = analyze_data_structure(data)
        st.subheader("Data Structure")
        st.json(structure)

        # Section 3: Recommendation
        chart_context = detect_chart_context(structure)
        st.subheader("Chart Recommendation")
        st.write(chart_context)

        # Section 4: Statistical Tests
        st.subheader("Statistical Relationships")
        test_results = perform_statistical_tests(data)
        st.json(test_results)

        # Section 5: Sentiment Input
        if model:
            user_text = st.text_input("Analyze Sentiment")
            if user_text:
                st.write(predict_sentiment(model, user_text))

if __name__ == "__main__":
    main()

!pip install -q streamlit pyngrok

# Save the full Streamlit app code into app.py
with open("app.py", "w") as f:
    f.write("""
import os
import json
import re
import streamlit as st
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency, pearsonr
import tensorflow as tf
from tensorflow.keras.models import load_model
from google.cloud import firestore

# ========================
# 1. Firestore Setup
# ========================
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/drive/My Drive/Data Sculpture/data-sculpture-ai-firebase-adminsdk-4ebrf-5409e1f2b5.json"

# ========================
# 2. Helper Functions
# ========================
def flatten_dict_items(data_items):
    flat_data = []
    for item in data_items:
        if isinstance(item, dict):
            flat_row = {}
            for k, v in item.items():
                if isinstance(v, dict):
                    for sub_k, sub_v in v.items():
                        flat_row[sub_k] = sub_v
                else:
                    flat_row[k] = v
            flat_data.append(flat_row)
    return flat_data

@st.cache_data(show_spinner=False)
def fetch_documents(collection_name="library"):
    db = firestore.Client()
    target_ids = {"xF3XvOMMx9RFQZsyNzbe", "enhanced_results_test_01", "enhanced_results_test_02"}
    docs = db.collection(collection_name).stream()
    results = []
    for doc in docs:
        if doc.id in target_ids:
            data = doc.to_dict()
            results.append({
                "doc_id": doc.id,
                "data": data,
                "processed": data.get("processed", False)
            })
    return results

def analyze_data_structure(data_items):
    structure = {
        'numeric_columns': [],
        'categorical_columns': [],
        'temporal_columns': [],
        'has_baseline': False,
        'data_distribution': {},
        'relationships': []
    }

    flat_data = flatten_dict_items(data_items)
    df = pd.DataFrame(flat_data)
    if df.empty:
        return structure

    for col in df.columns:
        col_data = df[col].dropna()
        # Skip if col_data contains lists to avoid unhashable errors
        if col_data.apply(lambda x: isinstance(x, list)).any():
            continue

        try:
            parsed_dates = pd.to_datetime(col_data, errors='coerce')
            if parsed_dates.notna().sum() > 0.5 * len(col_data):
                structure['temporal_columns'].append(col)
                continue
        except:
            pass

        if pd.api.types.is_numeric_dtype(col_data):
            structure['numeric_columns'].append(col)
            structure['data_distribution'][col] = {
                "mean": float(col_data.mean()),
                "std": float(col_data.std()),
                "min": float(col_data.min()),
                "max": float(col_data.max())
            }
        elif pd.api.types.is_object_dtype(col_data):
            if col_data.nunique() < 10:
                structure['categorical_columns'].append(col)
                if col_data.nunique() == 2:
                    structure['has_baseline'] = True

    num_cols = structure['numeric_columns']
    for i in range(len(num_cols)):
        for j in range(i + 1, len(num_cols)):
            col1, col2 = num_cols[i], num_cols[j]
            try:
                r, p = pearsonr(df[col1].dropna(), df[col2].dropna())
                if p < 0.05:
                    structure['relationships'].append({
                        "columns": (col1, col2),
                        "correlation": round(r, 3),
                        "p_value": round(p, 4)
                    })
            except:
                continue

    return structure

def detect_chart_context(structure):
    num = structure['numeric_columns']
    cat = structure['categorical_columns']
    time = structure['temporal_columns']

    context = {
        "recommended_chart": None,
        "purpose": None,
        "reason": ""
    }

    if time and num:
        context["recommended_chart"] = "line"
        context["purpose"] = "trend over time"
        context["reason"] = f"Time column '{time[0]}' and numeric column '{num[0]}' detected"

    elif len(num) >= 2:
        context["recommended_chart"] = "scatter"
        context["purpose"] = "correlation"
        context["reason"] = f"Two numeric columns '{num[0]}' and '{num[1]}' detected"

    elif cat and num:
        context["recommended_chart"] = "bar"
        context["purpose"] = "comparison"
        context["reason"] = f"Categorical column '{cat[0]}' and numeric column '{num[0]}' detected"

    elif len(cat) >= 2:
        context["recommended_chart"] = "heatmap"
        context["purpose"] = "cross-category analysis"
        context["reason"] = f"Two categorical columns '{cat[0]}' and '{cat[1]}' detected"

    elif num:
        context["recommended_chart"] = "histogram"
        context["purpose"] = "distribution"
        context["reason"] = f"One numeric column '{num[0]}' detected"

    elif cat:
        context["recommended_chart"] = "pie"
        context["purpose"] = "proportion"
        context["reason"] = f"One categorical column '{cat[0]}' detected"

    else:
        context["recommended_chart"] = "table"
        context["purpose"] = "raw data"
        context["reason"] = "Unable to infer a strong visualization structure"

    return context

def perform_statistical_tests(data_items):
    try:
        df = pd.DataFrame(flatten_dict_items(data_items))
        result = {}

        # Avoid columns with list types in statistical tests
        df = df.loc[:, ~df.applymap(lambda x: isinstance(x, list)).any()]

        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

        for i, col1 in enumerate(df.columns):
            for col2 in df.columns[i+1:]:
                if col1 == col2 or df[col1].nunique() < 2 or df[col2].nunique() < 2:
                    continue

                if col1 in numeric_cols and col2 in numeric_cols:
                    stat, p = pearsonr(df[col1].dropna(), df[col2].dropna())
                    result[f"{col1} vs {col2}"] = {
                        "test": "Pearson Correlation",
                        "p_value": round(p, 4),
                        "significant": p < 0.05
                    }

                elif col1 in numeric_cols and col2 in categorical_cols:
                    groups = [df[df[col2] == val][col1].dropna() for val in df[col2].unique()]
                    if len(groups) == 2:
                        stat, p = ttest_ind(groups[0], groups[1], equal_var=False)
                        result[f"{col1} ~ {col2}"] = {
                            "test": "T-Test",
                            "p_value": round(p, 4),
                            "significant": p < 0.05
                        }

                elif col1 in categorical_cols and col2 in categorical_cols:
                    contingency = pd.crosstab(df[col1], df[col2])
                    if contingency.shape[0] > 1 and contingency.shape[1] > 1:
                        stat, p, _, _ = chi2_contingency(contingency)
                        result[f"{col1} vs {col2}"] = {
                            "test": "Chi-Square",
                            "p_value": round(p, 4),
                            "significant": p < 0.05
                        }

        return result
    except Exception as e:
        return {"error": str(e)}

def load_sentiment_model(path="/content/drive/My Drive/Data Sculpture/sentiment_classifier.keras"):
    if not os.path.exists(path):
        st.warning(f"Model file not found at {path}. Please train it first or upload the file.")
        return None
    try:
        return tf.keras.models.load_model(path)
    except Exception as e:
        st.error(f"Model load error: {e}")
        return None

def predict_sentiment(model, text):
    text_tensor = tf.constant([text])
    probs = model.predict(text_tensor)[0]
    classes = ['Negative', 'Neutral', 'Positive']
    return dict(zip(classes, np.round(probs, 3)))

# ========================
# 3. Streamlit App Logic
# ========================
def main():
    st.title("ðŸ“Š Data Sculpture: AI-Powered Insights & Visualization")

    model = load_sentiment_model()
    hypothesis = st.sidebar.text_area(
        "Enter your hypothesis question:",
        "Does feature1 correlate with feature2?"
    )

    selected_columns = st.sidebar.multiselect(
        "Select columns to analyze:",
        options=["feature1", "feature2", "feature3", "feature4", "feature5", "feature6"],
        default=["feature1", "feature2"]
    )

    test_type = st.sidebar.selectbox(
        "Select statistical test:",
        ["Pearson Correlation", "T-Test", "Chi-Square"]
    )

    sentiment_filter = st.sidebar.multiselect(
        "Filter by sentiment:",
        options=["Positive", "Neutral", "Negative"],
        default=["Positive", "Neutral", "Negative"]
    )

    confidence_level = st.sidebar.slider(
        "Minimum confidence level:",
        min_value=0.0, max_value=1.0, value=0.7
    )

    if st.button("Fetch Firestore Data"):
        data = fetch_documents("library")
        st.success(f"Fetched {len(data)} documents.")
        st.json(data)

        flat_data = flatten_dict_items([doc.get("data", {}) for doc in data])
        st.subheader("ðŸ” Data Structure")
        structure = analyze_data_structure(flat_data)
        st.json(structure)

        st.subheader("ðŸ“ˆ Chart Recommendation")
        chart = detect_chart_context(structure)
        st.json(chart)

        st.subheader("ðŸ“Š Statistical Analysis")
        stats = perform_statistical_tests(flat_data)
        st.json(stats)

    st.subheader("ðŸ§  Sentiment Analysis")
    user_input = st.text_input("Enter a comment or review:")
    if user_input and model:
        result = predict_sentiment(model, user_input)
        st.write(result)

if __name__ == "__main__":
    main()

""")

from pyngrok import ngrok
import time
import os

# Kill existing tunnels (if any)
ngrok.kill()

# Start a new tunnel on port 8501
public_url = ngrok.connect(8501)
print("ðŸ”— Streamlit App URL:", public_url)

# Run the Streamlit app (in background)
os.system("streamlit run app.py &")
time.sleep(5)  # Wait for it to initialize
